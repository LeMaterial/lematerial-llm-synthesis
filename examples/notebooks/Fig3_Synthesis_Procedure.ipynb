{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Script to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_synthesis.utils.style_utils import get_cmap, get_palette, set_style\n",
    "\n",
    "cmap = get_cmap()\n",
    "palette = get_palette()\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"/Users/magdalenalederbauer/Code/lematerial-llm-synthesis/examples/notebooks/dummy_data.csv\"\n",
    ")\n",
    "df.columns\n",
    "\n",
    "# Find the 5 most common synthesis methods\n",
    "synthesis_counts = df[\"synthesis_type\"].value_counts()\n",
    "top_5_synthesis = synthesis_counts.head(5).index.tolist()\n",
    "\n",
    "# Filter data for top 5 synthesis methods\n",
    "df_filtered = df[df[\"synthesis_type\"].isin(top_5_synthesis)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average scores for each synthesis method\n",
    "score_columns = [\n",
    "    \"material_extraction_score_llm\",\n",
    "    \"process_steps_score_llm\",\n",
    "    \"conditions_extraction_score_llm\",\n",
    "]\n",
    "averages = df_filtered.groupby(\"synthesis_type\")[score_columns].mean()\n",
    "\n",
    "\n",
    "# Function to plot bar chart for a specific score\n",
    "def plot_bar(df, score_column):\n",
    "    # Find the 5 most common synthesis methods\n",
    "    synthesis_counts = df[\"synthesis_type\"].value_counts()\n",
    "    top_5_synthesis = synthesis_counts.head(5).index.tolist()\n",
    "\n",
    "    # Filter data for top 5 synthesis methods\n",
    "    df_filtered = df[df[\"synthesis_type\"].isin(top_5_synthesis)]\n",
    "\n",
    "    # Calculate average scores for each synthesis method\n",
    "    averages = (\n",
    "        df_filtered.groupby(\"synthesis_type\")[score_column].mean().sort_index()\n",
    "    )\n",
    "\n",
    "    # Color mapping for different scores\n",
    "    color_map = {\n",
    "        \"material_extraction_score_llm\": palette[0],\n",
    "        \"process_steps_score_llm\": palette[1],\n",
    "        \"conditions_extraction_score_llm\": palette[2],\n",
    "        \"material_extraction_score_human\": palette[3],\n",
    "        \"process_steps_score_human\": palette[4],\n",
    "        \"conditions_extraction_score_human\": palette[5],\n",
    "    }\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    bars = plt.bar(\n",
    "        averages.index,\n",
    "        averages.values,\n",
    "        color=color_map[score_column],\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "    # Format the title\n",
    "    title = score_column.replace(\"_\", \" \").title()\n",
    "    plt.xlabel(\"Synthesis Method\", fontsize=12)\n",
    "    plt.ylabel(title, fontsize=12)\n",
    "    plt.ylim(0, 5)\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        height = round(height, 2)\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create the 3 separate plots\n",
    "plot_bar(df, \"material_extraction_score_llm\")\n",
    "plot_bar(df, \"process_steps_score_llm\")\n",
    "plot_bar(df, \"conditions_extraction_score_llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_comparison(df, llm_column, human_column, llm_color, human_color):\n",
    "    # Find the 5 most common synthesis methods\n",
    "    synthesis_counts = df[\"synthesis_type\"].value_counts()\n",
    "    top_5_synthesis = synthesis_counts.head(5).index.tolist()\n",
    "\n",
    "    # Filter data for top 5 synthesis methods\n",
    "    df_filtered = df[df[\"synthesis_type\"].isin(top_5_synthesis)]\n",
    "\n",
    "    # Calculate average scores for each synthesis method\n",
    "    llm_averages = (\n",
    "        df_filtered.groupby(\"synthesis_type\")[llm_column].mean().sort_index()\n",
    "    )\n",
    "    human_averages = (\n",
    "        df_filtered.groupby(\"synthesis_type\")[human_column].mean().sort_index()\n",
    "    )\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(3, 3))\n",
    "\n",
    "    # Set the width of bars and positions\n",
    "    bar_width = 0.4\n",
    "    x_pos = np.arange(len(llm_averages))\n",
    "\n",
    "    # Create bars\n",
    "    bars1 = plt.bar(\n",
    "        x_pos - bar_width / 2,\n",
    "        llm_averages.values,\n",
    "        bar_width,\n",
    "        label=\"LLM\",\n",
    "        color=llm_color,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "    bars2 = plt.bar(\n",
    "        x_pos + bar_width / 2,\n",
    "        human_averages.values,\n",
    "        bar_width,\n",
    "        label=\"Human\",\n",
    "        color=human_color,\n",
    "        linewidth=0.5,\n",
    "    )\n",
    "\n",
    "    # Format the plot\n",
    "    base_title = llm_column.replace(\"_llm\", \"\").replace(\"_\", \" \").title()\n",
    "    plt.xlabel(\"Synthesis Method\", fontsize=12)\n",
    "    plt.ylabel(base_title, fontsize=12)\n",
    "    plt.ylim(0, 5)\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Set x-axis labels\n",
    "    plt.xticks(x_pos, llm_averages.index, rotation=45)\n",
    "\n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        height = round(height, 2)\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        height = round(height, 2)\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend(loc=\"upper right\", fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Material extraction comparison\n",
    "plot_bar_comparison(\n",
    "    df,\n",
    "    llm_column=\"material_extraction_score_llm\",\n",
    "    human_column=\"material_extraction_score_human\",\n",
    "    llm_color=palette[0],\n",
    "    human_color=palette[1],\n",
    ")\n",
    "\n",
    "# # Process steps comparison\n",
    "plot_bar_comparison(\n",
    "    df,\n",
    "    llm_column=\"process_steps_score_llm\",\n",
    "    human_column=\"process_steps_score_human\",\n",
    "    llm_color=palette[2],\n",
    "    human_color=palette[3],\n",
    ")\n",
    "\n",
    "# # Conditions extraction comparison\n",
    "plot_bar_comparison(\n",
    "    df,\n",
    "    llm_column=\"conditions_extraction_score_llm\",\n",
    "    human_column=\"conditions_extraction_score_human\",\n",
    "    llm_color=palette[4],\n",
    "    human_color=palette[5],\n",
    ")\n",
    "\n",
    "# # Equipment extraction comparison\n",
    "plot_bar_comparison(\n",
    "    df,\n",
    "    llm_column=\"equipment_extraction_score_llm\",\n",
    "    human_column=\"equipment_extraction_score_human\",\n",
    "    llm_color=palette[2],\n",
    "    human_color=palette[5],\n",
    ")\n",
    "\n",
    "# # Semantic accuracy comparison\n",
    "plot_bar_comparison(\n",
    "    df,\n",
    "    llm_column=\"semantic_accuracy_score_llm\",\n",
    "    human_column=\"semantic_accuracy_score_human\",\n",
    "    llm_color=palette[2],\n",
    "    human_color=palette[5],\n",
    ")\n",
    "\n",
    "# # Format compliance comparison\n",
    "plot_bar_comparison(\n",
    "    df,\n",
    "    llm_column=\"format_compliance_score_llm\",\n",
    "    human_column=\"format_compliance_score_human\",\n",
    "    llm_color=palette[0],\n",
    "    human_color=palette[2],\n",
    ")\n",
    "\n",
    "# # # Overall score comparison\n",
    "# plot_bar_comparison(\n",
    "#     df,\n",
    "#     llm_column=\"overall_score_llm\",\n",
    "#     human_column=\"overall_score_human\",\n",
    "#     llm_color=palette[0],\n",
    "#     human_color=palette[2],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from llm_synthesis.utils.style_utils import get_cmap, set_style\n",
    "\n",
    "\n",
    "def load_result_json(file_path: str) -> list[dict[str, Any]]:\n",
    "    \"\"\"Load and parse result.json file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_synthesis_data(synthesis: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"Extract evaluation scores and metadata.\"\"\"\n",
    "    data = {\n",
    "        \"scores\": {},\n",
    "        \"synthesis_method\": None,\n",
    "        \"target_compound_type\": None,\n",
    "        \"material\": None,\n",
    "    }\n",
    "    if \"evaluation\" in synthesis and \"scores\" in synthesis[\"evaluation\"]:\n",
    "        for key, value in synthesis[\"evaluation\"][\"scores\"].items():\n",
    "            if key.endswith(\"_score\"):\n",
    "                data[\"scores\"][key] = float(value)\n",
    "    if \"synthesis\" in synthesis:\n",
    "        data[\"synthesis_method\"] = synthesis[\"synthesis\"].get(\n",
    "            \"synthesis_method\"\n",
    "        )\n",
    "        data[\"target_compound_type\"] = synthesis[\"synthesis\"].get(\n",
    "            \"target_compound_type\"\n",
    "        )\n",
    "    data[\"material\"] = synthesis.get(\"material\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_subdirectory(subdir_path: str) -> dict[str, Any]:\n",
    "    \"\"\"Process a single subdirectory.\"\"\"\n",
    "    result_file = os.path.join(subdir_path, \"result.json\")\n",
    "    syntheses = load_result_json(result_file)\n",
    "    if not syntheses:\n",
    "        return {}\n",
    "\n",
    "    all_scores, methods, types, materials = [], [], [], []\n",
    "    for s in syntheses:\n",
    "        data = extract_synthesis_data(s)\n",
    "        if data[\"evaluation\"] is None:\n",
    "            continue\n",
    "        if data[\"scores\"]:\n",
    "            all_scores.append(data[\"scores\"])\n",
    "            methods.append(data[\"synthesis_method\"])\n",
    "            types.append(data[\"target_compound_type\"])\n",
    "            materials.append(data[\"material\"])\n",
    "\n",
    "    if not all_scores:\n",
    "        return {}\n",
    "\n",
    "    all_categories = {k for scores in all_scores for k in scores.keys()}\n",
    "    avg_scores = {\n",
    "        cat: np.nanmean([s.get(cat, np.nan) for s in all_scores])\n",
    "        for cat in all_categories\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"subdir_name\": os.path.basename(subdir_path),\n",
    "        \"scores\": avg_scores,\n",
    "        \"synthesis_count\": len(all_scores),\n",
    "        \"synthesis_methods\": methods,\n",
    "        \"target_compound_types\": types,\n",
    "        \"materials\": materials,\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_metadata(results: list[dict[str, Any]]) -> dict[str, Any]:\n",
    "    \"\"\"Analyze metadata across all results.\"\"\"\n",
    "    all_methods = [\n",
    "        m for r in results if r for m in r.get(\"synthesis_methods\", []) if m\n",
    "    ]\n",
    "    all_types = [\n",
    "        t for r in results if r for t in r.get(\"target_compound_types\", []) if t\n",
    "    ]\n",
    "    return {\n",
    "        \"synthesis_methods\": dict(Counter(all_methods)),\n",
    "        \"target_compound_types\": dict(Counter(all_types)),\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_scores_by_category(\n",
    "    my_dir: str, results: list[dict[str, Any]], category_field: str\n",
    ") -> dict[str, dict[str, float]]:\n",
    "    \"\"\"Calculate average scores grouped by a specific category.\"\"\"\n",
    "    category_data = {}\n",
    "    for result in filter(None, results):\n",
    "        subdir_path = os.path.join(my_dir, result[\"subdir_name\"])\n",
    "        syntheses = load_result_json(os.path.join(subdir_path, \"result.json\"))\n",
    "        for s in syntheses:\n",
    "            data = extract_synthesis_data(s)\n",
    "            category = data.get(category_field)\n",
    "            if category and data[\"scores\"]:\n",
    "                category_data.setdefault(category, []).append(data[\"scores\"])\n",
    "\n",
    "    category_averages = {}\n",
    "    for category, score_lists in category_data.items():\n",
    "        all_score_types = {k for scores in score_lists for k in scores.keys()}\n",
    "        avg_scores = {\n",
    "            st: np.nanmean([s.get(st, np.nan) for s in score_lists])\n",
    "            for st in all_score_types\n",
    "        }\n",
    "        category_averages[category] = {\n",
    "            k: v for k, v in avg_scores.items() if not np.isnan(v)\n",
    "        }\n",
    "    return category_averages\n",
    "\n",
    "\n",
    "def plot_scores_by_category(\n",
    "    scores_by_category: dict[str, dict[str, float]],\n",
    "    category_name: str,\n",
    "    counts: dict[str, int],\n",
    "    cmap,\n",
    "):\n",
    "    \"\"\"Generate and save bar plots for each score type.\"\"\"\n",
    "    df = pd.DataFrame.from_dict(scores_by_category, orient=\"index\")\n",
    "    if not counts:\n",
    "        return\n",
    "\n",
    "    norm = plt.Normalize(\n",
    "        vmin=min(counts.values()) or 0, vmax=max(counts.values()) or 1\n",
    "    )\n",
    "\n",
    "    for score_column in df.columns:\n",
    "        fig, ax = plt.subplots()\n",
    "        sorted_scores = df[score_column].sort_values(ascending=False)\n",
    "        bar_colors = [\n",
    "            cmap(norm(counts.get(cat, 0))) for cat in sorted_scores.index\n",
    "        ]\n",
    "\n",
    "        sorted_scores.plot(kind=\"bar\", ax=ax, color=bar_colors)\n",
    "\n",
    "        ax.set_title(\n",
    "            f\"Average {score_column.replace('_', ' ').title()} by {category_name.replace('_', ' ').title()}\"\n",
    "        )\n",
    "        ax.set_xlabel(category_name.replace(\"_\", \" \").title())\n",
    "        ax.set_ylabel(\"Average Score\")\n",
    "        ax.set_ylim(0, 5)\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, ax=ax)\n",
    "        cbar.set_label(\"Number of Entries\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # plt.savefig(f\"{category_name}_{score_column}_plot_colored.png\")\n",
    "        # plt.close(fig)\n",
    "\n",
    "\n",
    "def plot_average_of_other_scores(\n",
    "    scores_by_category: dict[str, dict[str, float]],\n",
    "    category_name: str,\n",
    "    counts: dict[str, int],\n",
    "    cmap,\n",
    "):\n",
    "    \"\"\"Plot the average of all scores, excluding 'overall_score'.\"\"\"\n",
    "    df = pd.DataFrame.from_dict(scores_by_category, orient=\"index\")\n",
    "    score_columns = [col for col in df.columns if col != \"overall_score\"]\n",
    "    if not score_columns or not counts:\n",
    "        return\n",
    "\n",
    "    df[\"average_of_other_scores\"] = df[score_columns].mean(axis=1)\n",
    "    norm = plt.Normalize(\n",
    "        vmin=min(counts.values()) or 0, vmax=max(counts.values()) or 1\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    sorted_scores = df[\"average_of_other_scores\"].sort_values(ascending=False)\n",
    "    bar_colors = [cmap(norm(counts.get(cat, 0))) for cat in sorted_scores.index]\n",
    "\n",
    "    sorted_scores.plot(kind=\"bar\", ax=ax, color=bar_colors)\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"Average of Other Scores by {category_name.replace('_', ' ').title()}\"\n",
    "    )\n",
    "    ax.set_xlabel(category_name.replace(\"_\", \" \").title())\n",
    "    ax.set_ylabel(\"Average Score\")\n",
    "    ax.set_ylim(0, 5)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, ax=ax)\n",
    "    cbar.set_label(\"Number of Entries\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file: str = \"synthesis_evaluation_results.csv\"\n",
    "MY_DIR = \"/Users/magdalenalederbauer/Code/lematerial-llm-synthesis/annotations/\"\n",
    "my_dir = MY_DIR\n",
    "\n",
    "\n",
    "subdirs = [\n",
    "    d for d in os.listdir(my_dir) if os.path.isdir(os.path.join(my_dir, d))\n",
    "]\n",
    "\n",
    "# results = [process_subdirectory(os.path.join(my_dir, s)) for s in subdirs]\n",
    "results = []\n",
    "for s in subdirs:\n",
    "    try:\n",
    "        results.append(process_subdirectory(os.path.join(my_dir, s)))\n",
    "    except Exception as e:\n",
    "        print(s)\n",
    "        print(e)\n",
    "\n",
    "metadata = analyze_metadata(results)\n",
    "\n",
    "method_scores = calculate_scores_by_category(\n",
    "    my_dir, results, \"synthesis_method\"\n",
    ")\n",
    "if method_scores:\n",
    "    plot_scores_by_category(\n",
    "        method_scores,\n",
    "        \"synthesis_method\",\n",
    "        metadata[\"synthesis_methods\"],\n",
    "        cmap,\n",
    "    )\n",
    "    plot_average_of_other_scores(\n",
    "        method_scores,\n",
    "        \"synthesis_method\",\n",
    "        metadata[\"synthesis_methods\"],\n",
    "        cmap,\n",
    "    )\n",
    "\n",
    "\n",
    "type_scores = calculate_scores_by_category(\n",
    "    my_dir, results, \"target_compound_type\"\n",
    ")\n",
    "if type_scores:\n",
    "    plot_scores_by_category(\n",
    "        type_scores,\n",
    "        \"target_compound_type\",\n",
    "        metadata[\"target_compound_types\"],\n",
    "        cmap,\n",
    "    )\n",
    "    plot_average_of_other_scores(\n",
    "        type_scores,\n",
    "        \"target_compound_type\",\n",
    "        metadata[\"target_compound_types\"],\n",
    "        cmap,\n",
    "    )\n",
    "\n",
    "print(\"Plot generation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
