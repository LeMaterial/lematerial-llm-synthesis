{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Script to plot relevant plots for Fig. 2 (NeurIPS AI4Mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from llm_synthesis.utils.style_utils import get_cmap, get_palette, set_style\n",
    "\n",
    "cmap = get_cmap()\n",
    "palette = get_palette()\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"LeMaterial/LeMat-Synth\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds[\"sample_for_evaluation\"].to_pandas()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"paper_published_date\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first 4 numbers of the paper_published_date\n",
    "df[\"paper_published_date\"] = df[\"paper_published_date\"].str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"source\"] is arxiv when paper_url contains arxiv.org, chemrxiv when paper_url contains chemrxiv.org, else it is \"omg24\"\n",
    "\n",
    "df[\"source\"] = df[\"paper_url\"].apply(\n",
    "    lambda x: \"arxiv\"\n",
    "    if \"arxiv.org\" in x\n",
    "    else \"chemrxiv\"\n",
    "    if \"chemrxiv.org\" in x\n",
    "    else \"omg24\"\n",
    ")\n",
    "\n",
    "df.groupby(\"source\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"material_category\", \"synthesis_method\", \"paper_published_date\", \"source\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.getcwd()\n",
    "\n",
    "# Define the full path for the output file\n",
    "file_path = os.path.join(output_dir, \"dataset_statistics_with_source.csv\")\n",
    "\n",
    "\n",
    "df[\n",
    "    [\"material_category\", \"synthesis_method\", \"paper_published_date\", \"source\"]\n",
    "].to_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_paper = load_dataset(\n",
    "    \"LeMaterial/LeMat-Synth-Papers\", subset=\"full\", split=\"arxiv\"\n",
    ")\n",
    "\n",
    "df_paper = ds_paper.to_pandas()\n",
    "\n",
    "df_paper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_folder = (\n",
    "    \"/Users/magdalenalederbauer/Code/lematerial-llm-synthesis/annotations\"\n",
    ")\n",
    "\n",
    "# for every subdir in annotation_folder\n",
    "for subdir in os.listdir(annotation_folder):\n",
    "    # id = name of subdir\n",
    "    id = subdir\n",
    "    id = id.replace(\"cond-mat.\", \"cond-mat/\")\n",
    "    synthesis_procedures_of_paper = df_paper[df_paper[\"id\"] == id]\n",
    "    url_of_paper = (\n",
    "        synthesis_procedures_of_paper[\"pdf_url\"]\n",
    "        .values[0]\n",
    "        .replace(\"https://\", \"\")\n",
    "    )\n",
    "    matched_lemat_synth_entry = df[df[\"paper_url\"].str.contains(url_of_paper)]\n",
    "    if len(matched_lemat_synth_entry) == 0:\n",
    "        continue\n",
    "    if (\n",
    "        matched_lemat_synth_entry[\"synthesized_material\"].values[0]\n",
    "        == \"No materials synthesized\"\n",
    "    ):\n",
    "        continue\n",
    "\n",
    "    # result_llm = subdir/result.json\n",
    "    result_llm = os.path.join(annotation_folder, subdir, \"result.json\")\n",
    "    result_human = os.path.join(annotation_folder, subdir, \"result_human.json\")\n",
    "\n",
    "    # load llm_ontology as json\n",
    "    llm_ontology = json.loads(open(result_llm).read())\n",
    "    try:\n",
    "        human_ontology = json.loads(open(result_human).read())\n",
    "    except FileNotFoundError:\n",
    "        # print(f\"No human ontology for {id}\")\n",
    "        # human ontology is a list of empty dicts in same format as llm_ontology\n",
    "        human_ontology = [{} for _ in llm_ontology]\n",
    "\n",
    "    for idx, (item_llm, item_human) in enumerate(\n",
    "        zip(llm_ontology, human_ontology)\n",
    "    ):\n",
    "        mat_name = item_llm[\"material\"]\n",
    "        synthesis = item_llm[\"synthesis\"]\n",
    "        evaluation_llm = item_llm[\"evaluation\"]\n",
    "        evaluation_human = item_human[\"evaluation\"] if item_human else None\n",
    "        # fill the first row of matched_lemat_synth_entry with the values\n",
    "        try:\n",
    "            matched_lemat_synth_entry.iloc[idx] = {\n",
    "                \"synthesized_material\": mat_name,\n",
    "                \"synthesis\": synthesis,\n",
    "                \"synthesis_extraction_performance_llm\": evaluation_llm,\n",
    "                \"synthesis_extraction_performance_human\": evaluation_human,\n",
    "            }\n",
    "        except Exception:\n",
    "            print(f\"Error filling row {idx} of {id}\")\n",
    "            print(matched_lemat_synth_entry)\n",
    "            print(mat_name)\n",
    "            print(synthesis)\n",
    "            print(evaluation_llm)\n",
    "            print(evaluation_human)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_ontology[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_lemat_synth_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
