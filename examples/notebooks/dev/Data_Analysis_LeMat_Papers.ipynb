{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "611431b4"
   },
   "source": [
    "# **LeMaterial/LeMat-Synth Dataset Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "bjdLvWqn9Qsn"
   },
   "source": [
    "This notebook is a v0 data analysis of LeMaterial/LeMat-Synth-Papers dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "ZusQk8gFcE3Q"
   },
   "source": [
    "## **Available Dataset Splits**\n",
    "\n",
    "- **arxiv**: ArXiv research papers\n",
    "- **chemrxiv**: ChemRxiv chemistry papers\n",
    "- **omg24**: OMG24 conference papers\n",
    "- **sample_for_evaluation**: Evaluation samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "28022a9c"
   },
   "source": [
    "## **Load libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "466351f3"
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import ast\n",
    "import re\n",
    "import string\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from llm_synthesis.utils.style_utils import get_palette, set_style\n",
    "\n",
    "set_style()\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Remove the limit from the number of displayed columns and rows. It helps to see the entire dataframe while printing it\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "a057afb2"
   },
   "source": [
    "## **Data set exploration**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "oghk3moqQ-fs"
   },
   "source": [
    "### **Understand the data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "3aOOslsbciNi"
   },
   "source": [
    "Examine the columns, data types, and basic statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cc11389",
    "outputId": "4a1cd78c-d05e-4c57-a2eb-b80be3c70ed6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"LeMaterial/LeMat-Synth-Papers\", split=\"sample_for_evaluation\"\n",
    ")\n",
    "\n",
    "data = dataset.to_pandas()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f05f5a8",
    "outputId": "1a84f5b1-28d9-43dc-e0d1-aeb848721e5b"
   },
   "outputs": [],
   "source": [
    "# Check the datatypes of each column.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "AntfBPSfQuMd",
    "outputId": "64b2ad74-89b9-4f36-991a-b755405450fa"
   },
   "outputs": [],
   "source": [
    "# Numerical statistics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "kKXkcQYf-PbC"
   },
   "source": [
    "### **Handle missing values**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "2qf-EAZ3-TTr"
   },
   "source": [
    "Identify and address any missing values in the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSK-RftW-dVl",
    "outputId": "44df09c3-2cc9-4061-acf6-1980db21aa61"
   },
   "outputs": [],
   "source": [
    "missing_percentages = data.isnull().sum()\n",
    "print(\"Percentage of missing values per column:\")\n",
    "print(missing_percentages)\n",
    "if \"images\" in data.columns:\n",
    "    data = data.drop(\"images\", axis=1)\n",
    "    print(\"\\n'images' column dropped.\")\n",
    "\n",
    "# Fill remaining missing values with \"N/A\"\n",
    "data = data.fillna(\"N/A\")\n",
    "print(\"\\nRemaining missing values filled with 'N/A'.\")\n",
    "\n",
    "# Verify that there are no more missing values\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "83f2aa89"
   },
   "source": [
    "## **Data Visualisation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "NRkaOk55HD4-"
   },
   "source": [
    "Create visualizations to understand the distribution and relationships within the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "a2a360cb"
   },
   "source": [
    "### **Summary statistics of all categorical variables**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "f8e2aedf",
    "outputId": "77c7444f-ca85-466b-f96a-ffcca12322c6"
   },
   "outputs": [],
   "source": [
    "# Explore basic summary statistics of categorical variables.\n",
    "data.describe(include=[\"object\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "AZPnZay_Sm78"
   },
   "source": [
    "### **Distribution of publications by date**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "NCQdtsZ9So_E",
    "outputId": "193f991b-5db7-4750-e33c-94aa0215ec88"
   },
   "outputs": [],
   "source": [
    "data[\"published_date\"] = pd.to_datetime(data[\"published_date\"], errors=\"coerce\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "data[\"published_year\"] = data[\"published_date\"].dt.year\n",
    "sns.countplot(data=data, x=\"published_year\")\n",
    "plt.title(\"Number of publications per year\")\n",
    "plt.xlabel(\"Year of publication\")\n",
    "plt.ylabel(\"Number of publications\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with missing year (due to coercion errors)\n",
    "yearly_data = data\n",
    "\n",
    "# Group by year and source and count the publications\n",
    "yearly_counts = (\n",
    "    yearly_data.groupby([\"published_year\", \"source\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "# Plotting the number of publications per year by source\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.lineplot(\n",
    "    data=yearly_counts, x=\"published_year\", y=\"count\", hue=\"source\", marker=\"o\"\n",
    ")\n",
    "plt.title(\"Number of Publications Per Year by Source\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Publications\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "EPcMvbYUzssT"
   },
   "source": [
    "**Observations**\n",
    "\n",
    "The dataset spans publication dates from 1996 to 2025, with a median publication year of 2019, indicating a concentration of more recent papers. The year 2023 has the highest number of publications in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "aloBAfZ537gV"
   },
   "source": [
    "### **Distribution of views, reads, citations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "bdm0bdvd38bT",
    "outputId": "75e734c9-d9ca-4d6e-eba4-6c6a52ec84ec"
   },
   "outputs": [],
   "source": [
    "# Convert relevant columns to numeric, coercing errors\n",
    "data[\"views_count\"] = pd.to_numeric(data[\"views_count\"], errors=\"coerce\")\n",
    "data[\"read_count\"] = pd.to_numeric(data[\"read_count\"], errors=\"coerce\")\n",
    "data[\"citation_count\"] = pd.to_numeric(data[\"citation_count\"], errors=\"coerce\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(data[\"views_count\"].dropna(), kde=True, bins=20)\n",
    "plt.title(\"Distribution of View Count\")\n",
    "plt.xlabel(\"Views\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.histplot(data[\"read_count\"].dropna(), kde=True, bins=20)\n",
    "plt.title(\"Distribution of Read Count\")\n",
    "plt.xlabel(\"Reads\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.histplot(data[\"citation_count\"].dropna(), kde=True, bins=20)\n",
    "plt.title(\"Distribution of Citation Count\")\n",
    "plt.xlabel(\"Citations\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "8tn0YFkrF_SF"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "The distributions of views, reads and citations show strongly right-skewed trends, indicating the presence of a few high-performing publications within a majority of low-impact ones.\n",
    "\n",
    "- Views: spread distribution with a mode around 500–1000 views; a few publications exceed 5000 views.\n",
    "\n",
    "- Reads: more concentrated distribution, with the majority of articles below 1,000 reads; rare extreme cases up to 8,000.\n",
    "\n",
    "- Citations: highly unbalanced distribution; the majority of publications have fewer than 10 citations, with very few exceeding 50.\n",
    "\n",
    "This asymmetry reflects a concentration of visibility and scientific impact on a small number of publications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "TVQeQtR8Tmgb"
   },
   "source": [
    "### **Frenquency of categories**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "id": "zcYitwPXTmM2",
    "outputId": "9f62ccb4-8c26-4749-f50b-1261757a800f"
   },
   "outputs": [],
   "source": [
    "# Function to clean and flatten category strings\n",
    "def clean_categories(categories_list):\n",
    "    cleaned = []\n",
    "    if isinstance(categories_list, str):\n",
    "        try:\n",
    "            # Attempt to safely evaluate the string as a Python literal (like a list)\n",
    "            categories_list = ast.literal_eval(categories_list)\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If evaluation fails, treat the string as a comma/semicolon separated list\n",
    "            categories_list = [\n",
    "                categories_list\n",
    "            ]  # Put the string in a list to process below\n",
    "\n",
    "    if isinstance(categories_list, list):\n",
    "        for category in categories_list:\n",
    "            if isinstance(category, str):  # Ensure the element is a string\n",
    "                # Clean special characters and unwanted spaces\n",
    "                # Split by comma and then by semicolon, strip whitespace, handle potential empty strings\n",
    "                for part in category.split(\",\"):\n",
    "                    for sub_part in part.split(\";\"):\n",
    "                        cleaned_category = re.sub(\n",
    "                            r\"[^a-zA-Z0-9\\s]\", \"\", sub_part\n",
    "                        ).strip()\n",
    "                        cleaned_category = cleaned_category.capitalize()\n",
    "                        # Eliminate generic or irrelevant categories (including \"Other\" and \"Na\")\n",
    "                        if (\n",
    "                            cleaned_category\n",
    "                            and cleaned_category.lower()\n",
    "                            not in [\n",
    "                                \"material\",\n",
    "                                \"science\",\n",
    "                                \"others\",\n",
    "                                \"n/a\",\n",
    "                                \"other\",\n",
    "                                \"na\",\n",
    "                            ]\n",
    "                        ):\n",
    "                            cleaned.append(cleaned_category)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "all_cleaned_categories = [\n",
    "    category\n",
    "    for sublist in data[\"categories\"].apply(clean_categories)\n",
    "    for category in sublist\n",
    "]\n",
    "category_counts = Counter(all_cleaned_categories)\n",
    "\n",
    "# Convert to DataFrame for better visualization with seaborn\n",
    "df_categories = pd.DataFrame(\n",
    "    category_counts.most_common(15), columns=[\"Category\", \"Count\"]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(data=df_categories, x=\"Count\", y=\"Category\")\n",
    "plt.title(\"Top 15 Publication Categories\")\n",
    "plt.xlabel(\"Number of Occurrences\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "C9QsKHQSHLEK"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "Publications are mainly concentrated in the fields of materials chemistry, materials science and inorganic chemistry, reflecting a strong focus on research into functional materials. Other dominant categories such as thin films, magnetic and nanomaterials indicate a focus on materials physics at the nanometric scale. This distribution highlights a marked specialisation at the interface between chemistry, physics and nanosciences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "id": "whCdPp4QUPNX"
   },
   "source": [
    "### **Top of keywords**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "mYhixKnOUSDH",
    "outputId": "ff1f8594-520b-45f3-df08-afa96797fdfa"
   },
   "outputs": [],
   "source": [
    "# Function to clean and format keywords\n",
    "def clean_keywords(keyword_entry):\n",
    "    cleaned_keywords = []\n",
    "    if isinstance(keyword_entry, str):\n",
    "        # Split by semicolon first to get individual keyword strings\n",
    "        keyword_strings = keyword_entry.split(\";\")\n",
    "        for kw_str in keyword_strings:\n",
    "            # Remove apostrophes and brackets, then strip whitespace\n",
    "            cleaned_kw = (\n",
    "                kw_str.replace(\"'\", \"\")\n",
    "                .replace(\"[\", \"\")\n",
    "                .replace(\"]\", \"\")\n",
    "                .strip()\n",
    "            )\n",
    "            # Join letters that might have been separated by apostrophes in the original data\n",
    "            # This assumes the apostrophes were just separators, and the remaining characters form the word\n",
    "            cleaned_kw = \"\".join(\n",
    "                cleaned_kw.split()\n",
    "            )  # Remove any remaining spaces within the word\n",
    "\n",
    "            if cleaned_kw:\n",
    "                cleaned_keywords.append(cleaned_kw)\n",
    "    return cleaned_keywords\n",
    "\n",
    "\n",
    "# Apply the cleaning function to the 'keywords' column and flatten the list of lists\n",
    "# Ensure to handle NaN values by converting to string and checking\n",
    "all_cleaned_keywords = [\n",
    "    keyword\n",
    "    for sublist in data[\"keywords\"].astype(str).apply(clean_keywords)\n",
    "    for keyword in sublist\n",
    "]\n",
    "\n",
    "# Join all cleaned keywords into a single string for the word cloud\n",
    "text = \" \".join(all_cleaned_keywords)\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(\n",
    "    text\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Keywords\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "jocc1-1dHhfc"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "The keyword map highlights themes that are strongly correlated with the main publication categories identified above. Terms such as ‘thin film’, ‘metal’, ‘self assembly’, ‘coordination polymer’, “MOF” and ‘supramolecular chemistry’ appear in large font, indicating their high frequency. These keywords confirm the predominance of topics in materials chemistry, inorganic chemistry, nanomaterials and supramolecular chemistry. The recurrence of terms such as ‘porosity’, ‘framework’, ‘host’, “luminescence” and ‘solid electrolytes’ reinforces the idea of a strong interest in functional materials, particularly porous or hybrid materials, for optoelectronic, catalytic or energy applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "id": "86xTjUxKgMAY",
    "outputId": "5afe6c60-76b2-4042-c61e-fe27d1555722"
   },
   "outputs": [],
   "source": [
    "# Count the frequency of each keyword\n",
    "keyword_counts = Counter(all_cleaned_keywords)\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "# Let's display the top 20 keywords for now\n",
    "top_n_keywords = 20\n",
    "df_top_keywords = pd.DataFrame(\n",
    "    keyword_counts.most_common(top_n_keywords), columns=[\"Keyword\", \"Count\"]\n",
    ")\n",
    "\n",
    "print(f\"Top {top_n_keywords} Keywords for the entire sample:\")\n",
    "display(df_top_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "r3riYtjZU_xM"
   },
   "source": [
    "## **Relationship between views, reads and citations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "ww97R0ILjTgN",
    "outputId": "744d8406-854f-49eb-ba1f-7611a95da563"
   },
   "outputs": [],
   "source": [
    "# Let's ensure these columns are numeric, coercing errors to NaN\n",
    "# and then fill NaNs if necessary (e.g., with 0 or the mean)\n",
    "for col in [\"views_count\", \"read_count\", \"citation_count\"]:\n",
    "    data[col] = pd.to_numeric(data[col], errors=\"coerce\")\n",
    "    data[col] = data[col].fillna(0)\n",
    "\n",
    "# Calculate aggregate statistics for the entire dataset\n",
    "# This is often done using .describe() or .agg()\n",
    "aggregated_stats = data[[\"views_count\", \"read_count\", \"citation_count\"]].agg(\n",
    "    [\"sum\", \"mean\", \"median\", \"min\", \"max\", \"std\"]\n",
    ")\n",
    "\n",
    "print(\"Aggregated Statistics for Views, Reads, and Citations:\")\n",
    "display(aggregated_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 557
    },
    "id": "AOncCVu5VtSo",
    "outputId": "23ccd3f0-99c4-4996-f5d6-1959c4071794"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=data, x=\"read_count\", y=\"citation_count\", alpha=0.6)\n",
    "plt.title(\"Reads vs. Citations\")\n",
    "plt.xlabel(\"Number of Reads\")\n",
    "plt.ylabel(\"Number of Citations\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "4Aj_XhV0aHx1",
    "outputId": "0faf4199-1b47-4da0-c674-a363b0b6b32c"
   },
   "outputs": [],
   "source": [
    "# Create heatmap with the citation count, read count and views count\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "sns.heatmap(\n",
    "    data[[\"views_count\", \"read_count\", \"citation_count\"]].corr(),\n",
    "    annot=True,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "\n",
    "plt.title(\"Correlation Heatmap of Impact Metrics\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "id": "QQ3WBbd_IIxT"
   },
   "source": [
    "## **Advanced data exploration**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "xmdjZ3QYIRI6"
   },
   "source": [
    "Perform a deeper dive into the dataset by exploring relationships between columns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "p1SXNrEIa0BK"
   },
   "source": [
    "### **Categories**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "Gna88vocZ8gi",
    "outputId": "b1ec35df-4fb2-4adc-8352-96cfd0640a08"
   },
   "outputs": [],
   "source": [
    "# Combine categories from all sources with category data\n",
    "all_categories = []\n",
    "# Using the main 'data' DataFrame and iterating through rows\n",
    "if \"data\" in globals():\n",
    "    for index, row in data.iterrows():\n",
    "        # Use the cleaned categories if available, otherwise clean them here\n",
    "        categories = row.get(\"cleaned_categories\")\n",
    "        if categories is None or not isinstance(categories, list):\n",
    "\n",
    "            def clean_single_category_entry(categories_list):\n",
    "                cleaned = []\n",
    "                if isinstance(categories_list, str):\n",
    "                    try:\n",
    "                        categories_list = ast.literal_eval(categories_list)\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        categories_list = [categories_list]\n",
    "\n",
    "                if isinstance(categories_list, list):\n",
    "                    for category in categories_list:\n",
    "                        if isinstance(category, str):\n",
    "                            for part in category.split(\",\"):\n",
    "                                for sub_part in part.split(\";\"):\n",
    "                                    cleaned_category = sub_part.strip()\n",
    "                                    if cleaned_category:\n",
    "                                        cleaned.append(cleaned_category)\n",
    "                return cleaned\n",
    "\n",
    "            categories = clean_single_category_entry(row[\"categories\"])\n",
    "\n",
    "        # Filter out 'N/A', 'Other', 'Others', 'NA' (case-insensitive)\n",
    "        filtered_categories = [\n",
    "            cat\n",
    "            for cat in categories\n",
    "            if isinstance(cat, str)\n",
    "            and cat.lower() not in [\"n/a\", \"other\", \"others\", \"na\"]\n",
    "        ]\n",
    "        all_categories.extend(filtered_categories)\n",
    "\n",
    "if all_categories:\n",
    "    # Count the frequency of each category\n",
    "    category_counts = Counter(all_categories)\n",
    "    category_df = pd.DataFrame(\n",
    "        category_counts.items(), columns=[\"Category\", \"Count\"]\n",
    "    )\n",
    "\n",
    "    # Create a treemap\n",
    "    fig = px.treemap(\n",
    "        category_df,\n",
    "        path=[\"Category\"],\n",
    "        values=\"Count\",\n",
    "        title=\"Treemap of Categories Across All Sources\",\n",
    "    )\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No category data available to create a treemap after filtering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "410b3e30",
    "outputId": "959b5ece-06da-48f1-913a-7581ce1da398"
   },
   "outputs": [],
   "source": [
    "# Function to clean and flatten category strings (copied from previous cell)\n",
    "def clean_categories(categories_list):\n",
    "    cleaned = []\n",
    "    if isinstance(categories_list, str):\n",
    "        try:\n",
    "            categories_list = ast.literal_eval(categories_list)\n",
    "        except (ValueError, SyntaxError):\n",
    "            categories_list = [categories_list]\n",
    "\n",
    "    if isinstance(categories_list, list):\n",
    "        for category in categories_list:\n",
    "            if isinstance(category, str):\n",
    "                for part in category.split(\",\"):\n",
    "                    for sub_part in part.split(\";\"):\n",
    "                        cleaned_category = re.sub(\n",
    "                            r\"[^a-zA-Z0-9\\s]\", \"\", sub_part\n",
    "                        ).strip()\n",
    "                        if cleaned_category:\n",
    "                            cleaned.append(cleaned_category)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Apply the cleaning function to the 'categories' column within this cell\n",
    "data[\"cleaned_categories\"] = data[\"categories\"].apply(clean_categories)\n",
    "\n",
    "# Flatten the list of lists and explicitly filter out 'N/A', 'Other', 'Others', and 'NA' (case-insensitive)\n",
    "all_cleaned_categories = [\n",
    "    category\n",
    "    for sublist in data[\"cleaned_categories\"]\n",
    "    for category in sublist\n",
    "    if category.lower() not in [\"n/a\", \"other\", \"others\", \"na\"]\n",
    "]\n",
    "\n",
    "# Count the occurrences of each filtered category\n",
    "category_counts = Counter(all_cleaned_categories)\n",
    "\n",
    "# Convert the counts to a pandas Series for easy plotting\n",
    "category_counts_series = pd.Series(category_counts).sort_values(ascending=False)\n",
    "\n",
    "# Plot a bar plot of the category counts\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = category_counts_series.plot(kind=\"bar\")\n",
    "\n",
    "plt.title(\"Number of Articles per Category\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.xticks(rotation=90, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add the exact number above each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        f\"{p.get_height()}\",\n",
    "        (p.get_x() + p.get_width() / 2.0, p.get_height()),\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        xytext=(0, 5),\n",
    "        textcoords=\"offset points\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "i1hlVU8CNxU4"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "This histogram reveals the most frequent categories in this dataset, as seen previously in the frequency of categories: materials chemistry/science, inorganic chemsitry and thin films.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4jqC09wKgBgy",
    "outputId": "18d7b537-f20f-4861-948e-66303b7f8500"
   },
   "outputs": [],
   "source": [
    "data2 = pd.DataFrame(data[\"categories\"])\n",
    "\n",
    "\n",
    "# Function to split and clean category strings - Modified to split by comma and semicolon\n",
    "def clean_categories(category_string):\n",
    "    if isinstance(category_string, str):\n",
    "        # Split by comma and then by semicolon, strip whitespace, handle potential empty strings\n",
    "        categories = []\n",
    "        for part in category_string.split(\",\"):\n",
    "            for sub_part in part.split(\";\"):\n",
    "                cleaned_cat = sub_part.strip()\n",
    "                # Convert to lowercase for case-insensitive filtering\n",
    "                if cleaned_cat and cleaned_cat.lower() not in [\n",
    "                    \"n/a\",\n",
    "                    \"other\",\n",
    "                    \"others\",\n",
    "                    \"na\",\n",
    "                ]:\n",
    "                    categories.append(cleaned_cat)\n",
    "        return categories\n",
    "    elif isinstance(category_string, list):\n",
    "        return [\n",
    "            cat.strip()\n",
    "            for cat in category_string\n",
    "            if isinstance(cat, str)\n",
    "            and cat.strip().lower() not in [\"n/a\", \"other\", \"others\", \"na\"]\n",
    "        ]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "# Apply the cleaning function\n",
    "data2[\"cleaned_categories\"] = data2[\"categories\"].apply(clean_categories)\n",
    "\n",
    "# --- 1. Select Top N Categories ---\n",
    "# Calculate the frequency of each category\n",
    "all_cats_list = [\n",
    "    cat for sublist in data2[\"cleaned_categories\"] for cat in sublist\n",
    "]\n",
    "category_counts = Counter(all_cats_list)\n",
    "\n",
    "# Define how many top categories you want to see\n",
    "N = 20\n",
    "top_categories = [cat for cat, count in category_counts.most_common(N)]\n",
    "top_categories = sorted(\n",
    "    top_categories\n",
    ")  # Sort alphabetically for consistent order\n",
    "\n",
    "print(\n",
    "    f\"Top {len(top_categories)} categories selected for the heatmap: {top_categories}\"\n",
    ")\n",
    "\n",
    "\n",
    "# --- 2. Create Co-occurrence Matrix for Top Categories ---\n",
    "# Initialize the DataFrame with dtype=float\n",
    "co_occurrence_matrix = pd.DataFrame(\n",
    "    0, index=top_categories, columns=top_categories, dtype=float\n",
    ")\n",
    "\n",
    "# Populate the matrix\n",
    "for categories_list in data2[\"cleaned_categories\"]:\n",
    "    # Filter the list to only include top categories\n",
    "    filtered_list = [cat for cat in categories_list if cat in top_categories]\n",
    "    # Use a set to get unique categories for co-occurrence counting within a single entry\n",
    "    for cat1 in set(filtered_list):\n",
    "        for cat2 in set(filtered_list):\n",
    "            if (\n",
    "                cat1 in co_occurrence_matrix.index\n",
    "                and cat2 in co_occurrence_matrix.columns\n",
    "            ):\n",
    "                co_occurrence_matrix.loc[cat1, cat2] += 1\n",
    "\n",
    "\n",
    "palette = get_palette()  # basically list of hex codes\n",
    "col_start = palette[2]  # Changed from palette[0]\n",
    "col_end = palette[3]  # Changed from palette[1]\n",
    "\n",
    "# Fix 2: Use LinearSegmentedColormap for smooth gradient\n",
    "colormap = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "    \"custom\", [\"#E7F0FF\", \"#448FF2\"], N=256\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. Plot the Heatmap (Full Matrix) ---\n",
    "plt.figure(figsize=(12, 10))\n",
    "heatmap = sns.heatmap(\n",
    "    co_occurrence_matrix,\n",
    "    annot=True,\n",
    "    # cmap is from palette[2] and palette[3]\n",
    "    cmap=colormap,\n",
    "    fmt=\".0f\",\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"label\": \"Co-occurrence Count\"},\n",
    ")\n",
    "\n",
    "# Improve plot aesthetics\n",
    "# plt.title(\"Co-occurrence Heatmap of Top 20 Categories\", fontsize=16, pad=20)\n",
    "# plt.xlabel(\"Category\", fontsize=12)\n",
    "# plt.ylabel(\"Category\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "# save as pdf\n",
    "plt.savefig(\"co_occurrence_heatmap.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "jws9h65fMF8E"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "This heatmap reveals the predominant interdisciplinary fields in materials science and chemistry. Inorganic chemistry is strongly linked to materials chemistry and solid state chemistry. Materials chemistry is also linked to physical chemistry. Finally, materials science co-occurs significantly with thin films and materials chemistry. The analysis highlights clear interactions between fundamental chemistry and materials applications. Plus, Supramolecular Chemisry (Org.) is correlated to Organic Chemistry.\n",
    "\n",
    "The diagonal indicates individual frequency, and off-diagonal values quantify interdisciplinary links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zIFW7wJw8Wta",
    "outputId": "46808d94-aad1-4826-81e6-37174bccd134"
   },
   "outputs": [],
   "source": [
    "# Create heatmap with the categories columns and source\n",
    "\n",
    "# Apply the cleaning function and create a list of (source, category) pairs\n",
    "source_category_pairs = []\n",
    "for index, row in data.iterrows():\n",
    "    source = row[\"source\"]\n",
    "    cleaned_cats = clean_categories(row[\"categories\"])\n",
    "    for category in cleaned_cats:\n",
    "        source_category_pairs.append({\"source\": source, \"category\": category})\n",
    "\n",
    "# Create a DataFrame from the pairs\n",
    "source_category_df = pd.DataFrame(source_category_pairs)\n",
    "\n",
    "# Create a crosstab of source and categories using the new DataFrame\n",
    "if not source_category_df.empty:\n",
    "    crosstab_data = pd.crosstab(\n",
    "        source_category_df[\"source\"], source_category_df[\"category\"]\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    sns.heatmap(crosstab_data, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(\"Heatmap of Source vs Categories\")\n",
    "    plt.xlabel(\"Categories\")\n",
    "    plt.ylabel(\"Source\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid category-source pairs found after cleaning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "r4ekO8kSY-vr",
    "outputId": "60d55905-727d-4563-d25e-1c2abb45534c"
   },
   "outputs": [],
   "source": [
    "# Function to clean and flatten category strings (more robust version)\n",
    "def clean_categories_for_counting(categories_list):\n",
    "    cleaned = []\n",
    "    if isinstance(categories_list, str):\n",
    "        try:\n",
    "            categories_list = ast.literal_eval(categories_list)\n",
    "        except (ValueError, SyntaxError):\n",
    "            categories_list = [categories_list]\n",
    "\n",
    "    if isinstance(categories_list, list):\n",
    "        for category in categories_list:\n",
    "            if isinstance(category, str):\n",
    "                for part in category.split(\",\"):\n",
    "                    for sub_part in part.split(\";\"):\n",
    "                        cleaned_category = re.sub(\n",
    "                            r\"[^a-zA-Z0-9\\s]\", \"\", sub_part\n",
    "                        ).strip()\n",
    "                        if cleaned_category:\n",
    "                            cleaned.append(cleaned_category)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Function to get the most common categories for a given source using the main data DataFrame\n",
    "def get_top_categories(df, source_name, n=10):\n",
    "    source_df = df[df[\"source\"] == source_name].copy()\n",
    "\n",
    "    # Apply the cleaning function to the 'categories' column for the specific source\n",
    "    source_df[\"cleaned_categories_list\"] = source_df[\"categories\"].apply(\n",
    "        clean_categories_for_counting\n",
    "    )\n",
    "\n",
    "    # Flatten the list of lists of cleaned categories\n",
    "    all_categories = [\n",
    "        cat\n",
    "        for sublist in source_df[\"cleaned_categories_list\"].dropna()\n",
    "        for cat in sublist\n",
    "    ]\n",
    "\n",
    "    # Count the frequency of each category\n",
    "    category_counts = Counter(all_categories)\n",
    "\n",
    "    # Return the most common categories\n",
    "    return category_counts.most_common(n)\n",
    "\n",
    "\n",
    "# Get top categories for each source\n",
    "top_n = 10\n",
    "if \"data\" in globals():\n",
    "    arxiv_top_categories = get_top_categories(data, \"arxiv\", n=top_n)\n",
    "    chemrxiv_top_categories = get_top_categories(data, \"chemrxiv\", n=top_n)\n",
    "    omg24_top_categories = get_top_categories(data, \"omg24\", n=top_n)\n",
    "\n",
    "    def plot_categories(category_freq, title):\n",
    "        if not category_freq:\n",
    "            print(f\"No category data to plot for {title}\")\n",
    "            return\n",
    "        categories, counts = zip(*category_freq)\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        sns.barplot(x=list(counts), y=list(categories))\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Count\")\n",
    "        plt.ylabel(\"Category\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Create bar charts for each source\n",
    "    plot_categories(\n",
    "        arxiv_top_categories, f\"Top {top_n} Categories in ArXiv Dataset\"\n",
    "    )\n",
    "    plot_categories(\n",
    "        chemrxiv_top_categories, f\"Top {top_n} Categories in ChemRxiv Dataset\"\n",
    "    )\n",
    "    plot_categories(\n",
    "        omg24_top_categories, f\"Top {top_n} Categories in OMG24 Dataset\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Error: 'data' DataFrame not found. Please run the data loading cells.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {
    "id": "VEzSiBrSbB9Q"
   },
   "source": [
    "### **Number of views**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eZ7AyDvOnIMu",
    "outputId": "37ab823d-6345-4e8f-d3fa-248467cac34a"
   },
   "outputs": [],
   "source": [
    "# Drop rows where 'views_count' or 'categories' are NaN, as they are not useful for this analysis\n",
    "data_cleaned = data.dropna(subset=[\"views_count\", \"categories\"]).copy()\n",
    "\n",
    "# Dictionary to store total views per category\n",
    "category_views = defaultdict(float)\n",
    "\n",
    "# Iterate through each row of the cleaned DataFrame\n",
    "for index, row in data_cleaned.iterrows():\n",
    "    views = row[\"views_count\"]\n",
    "    # Only process if views is a valid number\n",
    "    if pd.notna(views):\n",
    "        categories = clean_categories(row[\"categories\"])\n",
    "        for category in categories:\n",
    "            category_views[category] += views\n",
    "\n",
    "# Convert the dictionary to a DataFrame for better manipulation and visualization\n",
    "df_category_views = pd.DataFrame(\n",
    "    category_views.items(), columns=[\"Category\", \"Total_Views\"]\n",
    ")\n",
    "\n",
    "# Sort categories by total views\n",
    "df_category_views = df_category_views.sort_values(\n",
    "    by=\"Total_Views\", ascending=False\n",
    ")\n",
    "\n",
    "# Select Top N categories for better readability (e.g., top 15)\n",
    "top_n = 15\n",
    "df_top_categories_views = df_category_views.head(top_n)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "    data=df_top_categories_views,\n",
    "    x=\"Total_Views\",\n",
    "    y=\"Category\",\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "plt.title(f\"Top {top_n} Categories by Total Views\")\n",
    "plt.xlabel(\"Total Number of Views\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Top {top_n} Categories by Total Views:\")\n",
    "print(df_top_categories_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "id": "Hx2lY6RZpX-h"
   },
   "source": [
    "## **Sources**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "id": "HTsEvTVTINYC"
   },
   "source": [
    "### **Number of views, reads and citations per source**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wpmHXezaIR_R",
    "outputId": "37cc17c5-4b05-40bf-d825-8b834878a8cb"
   },
   "outputs": [],
   "source": [
    "# Create a copy and fill missing values with 0 for aggregation\n",
    "df_aggregated_counts = data.copy()\n",
    "df_aggregated_counts[[\"views_count\", \"read_count\", \"citation_count\"]] = (\n",
    "    df_aggregated_counts[\n",
    "        [\"views_count\", \"read_count\", \"citation_count\"]\n",
    "    ].fillna(0)\n",
    ")\n",
    "\n",
    "# Group by source and calculate the total for each metric\n",
    "total_counts_by_source = (\n",
    "    df_aggregated_counts.groupby(\"source\")[\n",
    "        [\"views_count\", \"read_count\", \"citation_count\"]\n",
    "    ]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Melt the DataFrame to long format for easier plotting with seaborn\n",
    "df_melted_counts = total_counts_by_source.melt(\n",
    "    id_vars=\"source\",\n",
    "    value_vars=[\"views_count\", \"read_count\", \"citation_count\"],\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Total_Count\",\n",
    ")\n",
    "\n",
    "# Define custom titles and labels\n",
    "metric_titles = {\n",
    "    \"views_count\": \"Total Number of Views by Source\",\n",
    "    \"read_count\": \"Total Number of Reads by Source\",\n",
    "    \"citation_count\": \"Total Number of Citations by Source\",\n",
    "}\n",
    "\n",
    "metric_ylabels = {\n",
    "    \"views_count\": \"Total Views\",\n",
    "    \"read_count\": \"Total Reads\",\n",
    "    \"citation_count\": \"Total Citations\",\n",
    "}\n",
    "\n",
    "# Create a separate plot for each metric\n",
    "for metric in [\"views_count\", \"read_count\", \"citation_count\"]:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        data=df_melted_counts[df_melted_counts[\"Metric\"] == metric],\n",
    "        x=\"source\",\n",
    "        y=\"Total_Count\",\n",
    "        palette=\"viridis\",\n",
    "    )\n",
    "    plt.title(metric_titles[metric])\n",
    "    plt.xlabel(\"Source\")\n",
    "    plt.ylabel(metric_ylabels[metric])\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "1FhBak0eiqwb",
    "outputId": "ad575009-b195-4fe1-8d23-82c4b27691a7"
   },
   "outputs": [],
   "source": [
    "# Create a pivot table.\n",
    "pivot_table_by_source_sum = data.pivot_table(\n",
    "    index=\"source\",\n",
    "    values=[\"views_count\", \"read_count\", \"citation_count\"],\n",
    "    aggfunc=\"sum\",\n",
    ")\n",
    "\n",
    "print(\"\\nPivot Table: Sum of Views, Reads, and Citations by Source:\")\n",
    "display(pivot_table_by_source_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {
    "id": "SfOf2NQVSdw2"
   },
   "source": [
    "### **Publication Trends Over time by source**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "LEKDSWVd5HQH",
    "outputId": "369d8e29-f9fd-4b09-fa5a-f5bee7b482bd"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_parquet(\"sample_for_evaluation.parquet\")\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        \"Please make sure 'sample_for_evaluation.parquet' is in the same directory.\"\n",
    "    )\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"published_date\": [\"2020\", \"2021-01-01T00:00:00Z\", \"2022\"],\n",
    "            \"source\": [\"arxiv\", \"chemrxiv\", \"omg24\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "data[\"published_year\"] = data[\"published_date\"].astype(str).str[:4]\n",
    "\n",
    "# Convert the extracted year to a numeric type, coercing errors to NaN\n",
    "data[\"published_year\"] = pd.to_numeric(data[\"published_year\"], errors=\"coerce\")\n",
    "\n",
    "# Drop rows where the year could not be parsed or is missing\n",
    "data_cleaned = data.dropna(subset=[\"published_year\"]).copy()\n",
    "\n",
    "# Convert the year column to integer type for clean plotting\n",
    "data_cleaned[\"published_year\"] = data_cleaned[\"published_year\"].astype(int)\n",
    "\n",
    "# Count the number of publications for each source for each year.\n",
    "current_year = pd.Timestamp.now().year\n",
    "valid_years_data = data_cleaned[\n",
    "    (data_cleaned[\"published_year\"] >= 1990)\n",
    "    & (data_cleaned[\"published_year\"] <= current_year + 5)\n",
    "].copy()\n",
    "\n",
    "# Check if all sources are present after cleaning and filtering\n",
    "print(\"Data points per source after cleaning and filtering:\")\n",
    "print(valid_years_data[\"source\"].value_counts())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Group by year and source, then unstack to get sources as columns\n",
    "publications_over_time = (\n",
    "    valid_years_data.groupby([\"published_year\", \"source\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Print the sources and the head of the aggregated data to verify all three are present\n",
    "print(\n",
    "    \"\\nUnique sources in the aggregated data:\",\n",
    "    publications_over_time.columns.tolist(),\n",
    ")\n",
    "print(\"\\nHead of publications_over_time DataFrame:\")\n",
    "print(publications_over_time.head())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create a line plot showing the number of publications over time for each source\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "if not publications_over_time.empty:\n",
    "    sns.lineplot(\n",
    "        data=publications_over_time, markers=True, dashes=False, linewidth=2.5\n",
    "    )\n",
    "\n",
    "    plt.title(\n",
    "        \"Publication Trends Over Time by Source\", fontsize=16, fontweight=\"bold\"\n",
    "    )\n",
    "    plt.xlabel(\"Publication Year\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Publications\", fontsize=12)\n",
    "    plt.legend(title=\"Source\", fontsize=10)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid publication data found to plot trends over time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {
    "id": "a1vPi_K7UbxS"
   },
   "source": [
    "### **Top words by source**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMTHDBX8Uemi",
    "outputId": "607e4df5-cdaf-4026-9785-099330c86c1a"
   },
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "try:\n",
    "    nltk.data.find(\"corpora/stopwords\")\n",
    "except LookupError:\n",
    "    nltk.download(\"stopwords\", quiet=True)\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt_tab\", quiet=True)\n",
    "\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "# Define a text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    try:\n",
    "        words = word_tokenize(text)\n",
    "    except LookupError:\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "        words = word_tokenize(text)\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Filter data for each source\n",
    "arxiv_df = data[data[\"source\"] == \"arxiv\"].copy()\n",
    "chemrxiv_df = data[data[\"source\"] == \"chemrxiv\"].copy()\n",
    "omg24_df = data[data[\"source\"] == \"omg24\"].copy()\n",
    "\n",
    "# Combine and preprocess text for each source\n",
    "arxiv_df[\"combined_text\"] = (\n",
    "    arxiv_df[\"title\"].fillna(\"\") + \" \" + arxiv_df[\"abstract\"].fillna(\"\")\n",
    ")\n",
    "arxiv_df[\"cleaned_text\"] = arxiv_df[\"combined_text\"].apply(preprocess_text)\n",
    "\n",
    "chemrxiv_df[\"combined_text\"] = (\n",
    "    chemrxiv_df[\"title\"].fillna(\"\") + \" \" + chemrxiv_df[\"abstract\"].fillna(\"\")\n",
    ")\n",
    "chemrxiv_df[\"cleaned_text\"] = chemrxiv_df[\"combined_text\"].apply(\n",
    "    preprocess_text\n",
    ")\n",
    "\n",
    "omg24_df[\"combined_text\"] = (\n",
    "    omg24_df[\"title\"].fillna(\"\") + \" \" + omg24_df[\"abstract\"].fillna(\"\")\n",
    ")\n",
    "omg24_df[\"cleaned_text\"] = omg24_df[\"combined_text\"].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Perform word frequency analysis for each source\n",
    "def get_word_frequencies(df, text_column=\"cleaned_text\", n=30):\n",
    "    if df.empty:\n",
    "        return []\n",
    "    all_words = \" \".join(df[text_column]).split()\n",
    "    word_counts = Counter(all_words)\n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "\n",
    "top_n = 30\n",
    "arxiv_word_freq = get_word_frequencies(arxiv_df, n=top_n)\n",
    "chemrxiv_word_freq = get_word_frequencies(chemrxiv_df, n=top_n)\n",
    "omg24_word_freq = get_word_frequencies(omg24_df, n=top_n)\n",
    "\n",
    "print(\"Top 30 words in ArXiv dataset:\")\n",
    "print(arxiv_word_freq)\n",
    "print(\"\\nTop 30 words in ChemRxiv dataset:\")\n",
    "print(chemrxiv_word_freq)\n",
    "print(\"\\nTop 30 words in OMG24 dataset:\")\n",
    "print(omg24_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VwG52rsmWihR",
    "outputId": "42a8ca22-a3a9-4461-aebb-e6e61d121e4e"
   },
   "outputs": [],
   "source": [
    "# Function to create a bar chart of word frequencies\n",
    "def plot_word_frequencies(word_freq, title):\n",
    "    words, counts = zip(*word_freq)\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(x=list(counts), y=list(words), palette=\"viridis\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Words\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create bar charts for each source\n",
    "plot_word_frequencies(arxiv_word_freq, \"Top 30 Most Frequent Words in ArXiv\")\n",
    "plot_word_frequencies(\n",
    "    chemrxiv_word_freq, \"Top 30 Most Frequent Words in ChemRxiv\"\n",
    ")\n",
    "plot_word_frequencies(omg24_word_freq, \"Top 30 Most Frequent Words in OMG24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d5HLey9sXTLx",
    "outputId": "d9dadb43-c230-4d6f-f56f-a0e9c9b46c06"
   },
   "outputs": [],
   "source": [
    "# Function to generate and display a word cloud\n",
    "def plot_word_cloud(word_freq, title):\n",
    "    if not word_freq:\n",
    "        print(\n",
    "            f\"No word frequency data available for '{title}'. Skipping word cloud generation.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=400, background_color=\"white\"\n",
    "    ).generate_from_frequencies(dict(word_freq))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate and display word clouds for each source\n",
    "plot_word_cloud(arxiv_word_freq, \"Word Cloud for ArXiv Dataset\")\n",
    "plot_word_cloud(chemrxiv_word_freq, \"Word Cloud for ChemRxiv Dataset\")\n",
    "plot_word_cloud(omg24_word_freq, \"Word Cloud for OMG24 Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {
    "id": "x1k7reRspaKL"
   },
   "source": [
    "### **Abstract lenght**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "hzWT9QYQacuR",
    "outputId": "59f38a21-5d3e-405f-8bcc-f0682b263055"
   },
   "outputs": [],
   "source": [
    "# Combine the dataframes\n",
    "if \"combined_df\" not in locals():\n",
    "    combined_df = pd.concat(\n",
    "        [\n",
    "            arxiv_df.assign(source=\"arxiv\"),\n",
    "            chemrxiv_df.assign(source=\"chemrxiv\"),\n",
    "            omg24_df.assign(source=\"omg24\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Calculate the length of abstracts\n",
    "combined_df[\"abstract_length\"] = combined_df[\"abstract\"].apply(\n",
    "    lambda x: len(str(x)) if pd.notna(x) and x != \"N/A\" else 0\n",
    ")\n",
    "\n",
    "# Create a distribution plot (density plot) of abstract lengths by source\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.kdeplot(\n",
    "    data=combined_df,\n",
    "    x=\"abstract_length\",\n",
    "    hue=\"source\",\n",
    "    fill=True,\n",
    "    common_norm=False,\n",
    ")\n",
    "plt.title(\"Distribution of Abstract Lengths by Source\")\n",
    "plt.xlabel(\"Abstract Length (Number of Characters)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {
    "id": "aDfxWZ3tog_W"
   },
   "source": [
    "**Observations:**\n",
    "\n",
    "This graph represents the distribution of abstract lengths (in number of characters) for three different sources: arXiv, chemrxiv, and omg24.\n",
    "\n",
    "- The distribution of abstract lengths for arXiv is unimodal and relatively symmetrical, centred around 800-1000 characters, suggesting a standardised or preferred length for scientific publications submitted to this repository.\n",
    "\n",
    "- The distribution for chemrxiv is also unimodal but wider and slightly asymmetrical (positive skewness), with a peak around 1200-1400 characters. This could indicate greater variability in abstract lengths for chemistry articles, perhaps due to specific disciplinary requirements or a greater diversity of topics requiring more or less detailed descriptions.\n",
    "\n",
    "- In contrast, the distribution of omg24 is bimodal, with a pronounced first peak around 0-200 characters and a second broader and lower peak around 800-1000 characters. The presence of the first peak at very short lengths is notable and could indicate the presence of empty abstracts, very short summaries, or missing/atypical data in this source. The second peak partially overlaps with the distribution of arXiv, suggesting that some omg24 abstracts share similar length characteristics with those of arXiv. The nature of ‘omg24’ is unknown, but this bimodality raises questions about the typology of the documents or their completeness.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
