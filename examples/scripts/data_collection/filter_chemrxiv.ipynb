{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# Load your datasets (as you did)\n",
    "dataset = load_dataset(\"amayuelas/lematerial-chemrxiv-filtered\")\n",
    "df = dataset[\"chemrxiv\"].to_pandas()\n",
    "\n",
    "dataset_upstream = load_dataset(\n",
    "    \"LeMaterial/LeMat-Synth\",\n",
    "    verification_mode=\"no_checks\",\n",
    ")\n",
    "df_upstream = dataset_upstream[\"chemrxiv\"].to_pandas()\n",
    "\n",
    "ids_to_keep = df[df[\"contains_recipe\"]][\"id\"].tolist()\n",
    "len(ids_to_keep)\n",
    "\n",
    "# 4. Create a new DatasetDict to store the filtered splits\n",
    "filtered_dataset_dict = DatasetDict()\n",
    "\n",
    "print(\n",
    "    \"Processing and filtering 'chemrxiv' split directly using dataset.filter().\"\n",
    ")\n",
    "\n",
    "# Get the 'chemrxiv' split directly from the DatasetDict\n",
    "chemrxiv_ds = dataset_upstream[\"chemrxiv\"]\n",
    "\n",
    "filtered_chemrxiv_ds = chemrxiv_ds.filter(\n",
    "    lambda example: example[\"id\"] in ids_to_keep,\n",
    "    num_proc=os.cpu_count(),  # Use all available CPU cores for parallelism\n",
    ")\n",
    "\n",
    "print(\"Filtering done for 'chemrxiv' split.\")\n",
    "\n",
    "print(\"Adding to DatasetDict\")\n",
    "# Add the filtered 'chemrxiv' split to your new DatasetDict\n",
    "filtered_dataset_dict[\"chemrxiv\"] = filtered_chemrxiv_ds\n",
    "\n",
    "print(f\"  Original 'chemrxiv' split size: {len(chemrxiv_ds)}\")\n",
    "print(f\"  Filtered 'chemrxiv' split size: {len(filtered_chemrxiv_ds)}\")\n",
    "\n",
    "\n",
    "# For all other splits, directly copy them to the new DatasetDict\n",
    "# These operations are fast as they are just references or shallow copies\n",
    "filtered_dataset_dict[\"arxiv\"] = dataset_upstream[\"arxiv\"]\n",
    "print(f\"  'arxiv' split size: {len(dataset_upstream['arxiv'])}\")\n",
    "filtered_dataset_dict[\"omg24\"] = dataset_upstream[\"omg24\"]\n",
    "print(f\"  'omg24' split size: {len(dataset_upstream['omg24'])}\")\n",
    "\n",
    "# To inspect your filtered_dataset_dict\n",
    "print(\"\\nFinal filtered_dataset_dict structure:\")\n",
    "print(filtered_dataset_dict)\n",
    "print(\"\\nExample from filtered 'chemrxiv' split:\")\n",
    "print(filtered_dataset_dict[\"chemrxiv\"][0])  # Access the first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "# Assuming filtered_dataset_dict is already defined from your previous steps\n",
    "\n",
    "print(\"Sampling 100 random entries from each split and adding 'source' column.\")\n",
    "\n",
    "# Create a list to hold the sampled datasets before concatenating them\n",
    "sampled_datasets_list = []\n",
    "splits_to_sample = [\n",
    "    \"arxiv\",\n",
    "    \"chemrxiv\",\n",
    "    \"omg24\",\n",
    "]  # Define the splits you want to sample from\n",
    "\n",
    "for split_name in splits_to_sample:\n",
    "    if split_name in filtered_dataset_dict:\n",
    "        current_split_ds = filtered_dataset_dict[split_name]\n",
    "        total_samples_in_split = len(current_split_ds)\n",
    "\n",
    "        if total_samples_in_split == 0:\n",
    "            print(\n",
    "                f\"  Warning: Split '{split_name}' is empty. Skipping sampling.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "            # Shuffle the dataset first to ensure random sampling\n",
    "            # Use a fixed seed for reproducibility if needed (e.g., seed=42)\n",
    "        shuffled_ds = current_split_ds.shuffle(seed=42)\n",
    "\n",
    "        # Determine the number of samples to take: 100 or the total available if less\n",
    "        num_samples_to_take = min(100, total_samples_in_split)\n",
    "\n",
    "        # Select the first `num_samples_to_take` entries from the shuffled dataset\n",
    "        sampled_entries = shuffled_ds.select(range(num_samples_to_take))\n",
    "\n",
    "        # Add the \"source\" column to the sampled entries\n",
    "        sampled_entries_with_source = sampled_entries.map(\n",
    "            lambda example: {\"source\": split_name},\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"  Sampled {num_samples_to_take} entries from '{split_name}' (original size: {total_samples_in_split}).\"\n",
    "        )\n",
    "        sampled_datasets_list.append(sampled_entries_with_source)\n",
    "    else:\n",
    "        print(\n",
    "            f\"  Warning: Split '{split_name}' not found in filtered_dataset_dict. Skipping.\"\n",
    "        )\n",
    "\n",
    "# Concatenate all sampled datasets into a new single Dataset for evaluation\n",
    "if sampled_datasets_list:\n",
    "    # Concatenate only if there are datasets to concatenate\n",
    "    from datasets import concatenate_datasets\n",
    "\n",
    "    combined_sampled_ds = concatenate_datasets(sampled_datasets_list)\n",
    "    filtered_dataset_dict[\"sample_for_evaluation\"] = combined_sampled_ds\n",
    "    print(\n",
    "        f\"\\nCreated 'sample_for_evaluation' split with {len(combined_sampled_ds)} total entries.\"\n",
    "    )\n",
    "    print(\"Example from 'sample_for_evaluation' with 'source' column:\")\n",
    "    print(filtered_dataset_dict[\"sample_for_evaluation\"][0])\n",
    "else:\n",
    "    print(\n",
    "        \"\\nNo samples were generated for 'sample_for_evaluation' as no valid splits were found or all were empty.\"\n",
    "    )\n",
    "\n",
    "print(\"\\nFinal filtered_dataset_dict structure:\")\n",
    "print(filtered_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset_dict.push_to_hub(\"LeMaterial/LeMat-Synth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"LeMaterial/LeMat-Synth\")\n",
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
